name: Ingest IFREMER NetCDFs (Indian Ocean 2025 only)

# Manual + scheduled run
on:
  workflow_dispatch: {}
  schedule:
    - cron: '0 2 * * *'  # daily at 02:00 UTC (edit as needed)

concurrency:
  group: ingest-ifremer-netcdfs
  cancel-in-progress: true

jobs:
  ingest:
    runs-on: ubuntu-latest
    timeout-minutes: 180
    env:
      # Required: set this secret in repo Settings -> Secrets and variables -> Actions
      DATABASE_URL: ${{ secrets.DATABASE_URL }}
      # Optional: set this secret if you want Gemini generation enabled
      GEMINI_API_KEY: ${{ secrets.GEMINI_API_KEY }}
      # Tunables (can be adjusted here or read from Python env)
      CI_MAX_YEARS: "1"
      MAX_SIZE_BYTES: "62914560"   # 60 MiB
      DOWNLOAD_CONCURRENCY: "3"
      DOWNLOAD_RETRIES: "4"

      # NEW: restrict to Indian Ocean 2025 only
      ROOT_URL: "https://data-argo.ifremer.fr/geo/indian_ocean/2025/"
      MIN_YEAR: "2025"

    steps:
      - name: Checkout repo
        uses: actions/checkout@v4

      - name: Set up Python 3.10
        uses: actions/setup-python@v4
        with:
          python-version: "3.10"

      - name: Cache pip
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }}
          restore-keys: |
            ${{ runner.os }}-pip-

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          if [ -f requirements.txt ]; then pip install -r requirements.txt; else echo "No requirements.txt found"; fi

      - name: Create data dir
        run: |
          mkdir -p ./data/netcdf

      - name: Show Python + env info
        run: |
          python -V
          echo "DATABASE_URL is set: ${DATABASE_URL:+yes}"
          echo "GEMINI_API_KEY is set: ${GEMINI_API_KEY:+yes}"
          echo "CI_MAX_YEARS=${CI_MAX_YEARS}"
          echo "MAX_SIZE_BYTES=${MAX_SIZE_BYTES}"
          echo "DOWNLOAD_CONCURRENCY=${DOWNLOAD_CONCURRENCY}"
          echo "DOWNLOAD_RETRIES=${DOWNLOAD_RETRIES}"
          echo "ROOT_URL=${ROOT_URL}"
          echo "MIN_YEAR=${MIN_YEAR}"

      - name: Run schema (idempotent)
        run: |
          python -m scripts.run_schema || true

      - name: Run orchestration (crawl/download/ingest)
        # scripts/run_once.py should read ROOT_URL/MIN_YEAR or fall back to defaults.
        env:
          # re-export so child processes see them
          ROOT_URL: ${{ env.ROOT_URL }}
          MIN_YEAR: ${{ env.MIN_YEAR }}
          CI_MAX_YEARS: ${{ env.CI_MAX_YEARS }}
          DATABASE_URL: ${{ secrets.DATABASE_URL }}
        run: |
          python -m scripts.run_once

      - name: Upload data artifact (optional)
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: ingest-logs-${{ github.run_id }}
          path: |
            ./*.log
            ./data/netcdf/**/*
